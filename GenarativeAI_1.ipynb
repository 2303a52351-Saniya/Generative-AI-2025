{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxtnxlbL7tBqTYsXHz2AQU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303a52351-Saniya/Generative-AI-2025/blob/main/GenarativeAI_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Write Python code from scratch to find error metrics of deep learning model. Actual\n",
        "values and deep learning model predicted values are shown in Table 1. Also compare the results\n",
        "with the outcomes of libraries"
      ],
      "metadata": {
        "id": "zo-IF8HyjFBl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9JCrZbpzfGr5",
        "outputId": "ed177b05-6a29-45c2-ddde-a182cb122715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Calculations:\n",
            "MAE: 0.4600\n",
            "MSE: 0.2460\n",
            "RMSE: 0.4960\n",
            "\n",
            "Library Calculations:\n",
            "MAE: 0.4600\n",
            "MSE: 0.2460\n",
            "RMSE: 0.4960\n",
            "\n",
            "Manual and library calculations match.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "y_actual = np.array([20, 30, 40, 50, 60])\n",
        "y_pred = np.array([20.5, 30.3, 40.2, 50.6, 60.7])\n",
        "\n",
        "mae_manual = np.mean(np.abs(y_actual - y_pred))\n",
        "\n",
        "mse_manual = np.mean((y_actual - y_pred) ** 2)\n",
        "\n",
        "\n",
        "rmse_manual = np.sqrt(mse_manual)\n",
        "\n",
        "mae_lib = mean_absolute_error(y_actual, y_pred)\n",
        "mse_lib = mean_squared_error(y_actual, y_pred)\n",
        "rmse_lib = np.sqrt(mse_lib)\n",
        "\n",
        "\n",
        "print(\"Manual Calculations:\")\n",
        "print(f\"MAE: {mae_manual:.4f}\")\n",
        "print(f\"MSE: {mse_manual:.4f}\")\n",
        "print(f\"RMSE: {rmse_manual:.4f}\")\n",
        "\n",
        "print(\"\\nLibrary Calculations:\")\n",
        "print(f\"MAE: {mae_lib:.4f}\")\n",
        "print(f\"MSE: {mse_lib:.4f}\")\n",
        "print(f\"RMSE: {rmse_lib:.4f}\")\n",
        "\n",
        "assert np.isclose(mae_manual, mae_lib), \"MAE mismatch!\"\n",
        "assert np.isclose(mse_manual, mse_lib), \"MSE mismatch!\"\n",
        "assert np.isclose(rmse_manual, rmse_lib), \"RMSE mismatch!\"\n",
        "\n",
        "print(\"\\nManual and library calculations match.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write python code from scratch to find evaluation metrics of deep learning model.\n",
        "Actual values and deep learning model predicted values are shown in Table\n",
        "2. Also compare the\n",
        "results with outcome of libraries"
      ],
      "metadata": {
        "id": "OUcAQRIsfx_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Actual and predicted values\n",
        "Y_actual = np.array([0, 1, 2, 0, 2, 0, 1, 2, 2])\n",
        "Y_pred = np.array([0, 1, 0, 0, 0, 1, 2, 0, 2])\n",
        "\n",
        "# Accuracy\n",
        "accuracy = np.mean(Y_actual == Y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Precision, Recall, and F1-Score for each class\n",
        "def precision_recall_f1(y_true, y_pred, class_label):\n",
        "    tp = np.sum((y_true == class_label) & (y_pred == class_label))\n",
        "    fp = np.sum((y_true != class_label) & (y_pred == class_label))\n",
        "    fn = np.sum((y_true == class_label) & (y_pred != class_label))\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return precision, recall, f1_score\n",
        "\n",
        "classes = np.unique(Y_actual)\n",
        "for class_label in classes:\n",
        "    precision, recall, f1_score = precision_recall_f1(Y_actual, Y_pred, class_label)\n",
        "    print(f\"Class {class_label} - Precision: {precision}, Recall: {recall}, F1-Score: {f1_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZznzawtfuvo",
        "outputId": "41e8a1bd-f755-42b3-b6b4-135db6de7908"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4444444444444444\n",
            "Class 0 - Precision: 0.4, Recall: 0.6666666666666666, F1-Score: 0.5\n",
            "Class 1 - Precision: 0.5, Recall: 0.5, F1-Score: 0.5\n",
            "Class 2 - Precision: 0.5, Recall: 0.25, F1-Score: 0.3333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Accuracy\n",
        "accuracy_lib = accuracy_score(Y_actual, Y_pred)\n",
        "print(f\"Accuracy using scikit-learn: {accuracy_lib}\")\n",
        "\n",
        "# Precision, Recall, and F1-Score for each class\n",
        "precision_lib = precision_score(Y_actual, Y_pred, average=None)\n",
        "recall_lib = recall_score(Y_actual, Y_pred, average=None)\n",
        "f1_score_lib = f1_score(Y_actual, Y_pred, average=None)\n",
        "\n",
        "for class_label in classes:\n",
        "    print(f\"Class {class_label} - Precision using scikit-learn: {precision_lib[class_label]}, Recall using scikit-learn: {recall_lib[class_label]}, F1-Score using scikit-learn: {f1_score_lib[class_label]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnompUmZqCW7",
        "outputId": "0eb0e3a7-e92a-4d7f-cffc-806313f9a708"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using scikit-learn: 0.4444444444444444\n",
            "Class 0 - Precision using scikit-learn: 0.4, Recall using scikit-learn: 0.6666666666666666, F1-Score using scikit-learn: 0.5\n",
            "Class 1 - Precision using scikit-learn: 0.5, Recall using scikit-learn: 0.5, F1-Score using scikit-learn: 0.5\n",
            "Class 2 - Precision using scikit-learn: 0.5, Recall using scikit-learn: 0.25, F1-Score using scikit-learn: 0.3333333333333333\n"
          ]
        }
      ]
    }
  ]
}